{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf6ecbe5",
   "metadata": {},
   "source": [
    "library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7965572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce2d22a",
   "metadata": {},
   "source": [
    "parameter & utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ddf034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--sampling_frequency',type=int, default=20000)\n",
    "parser.add_argument('--lr_length',type=int, default=1000, help='length of low-resolution signal')\n",
    "parser.add_argument('--hr_length',type=int, default=10000)\n",
    "parser.add_argument('--train_x_img_dir',type=str, default='./data/train/x')\n",
    "parser.add_argument('--train_y_img_dir',type=str, default='./data/train/y')\n",
    "parser.add_argument('--test_x_img_dir',type=str, default='./data/test/x')\n",
    "parser.add_argument('--test_y_img_dir',type=str, default='./data/test/y')\n",
    "parser.add_argument('--n_samples',type=int, default=2)\n",
    "parser.add_argument('--patch_size',type=int, default=128)\n",
    "parser.add_argument('--stride',type=int, default=96)\n",
    "\n",
    "parser.add_argument('--epochs',type=int, default=10)\n",
    "parser.add_argument('--lr',type=float, default=1e-4)\n",
    "parser.add_argument('--early_stop',type=int, default=20, help='early stop_patience')\n",
    "parser.add_argument('--batch_size',type=int, default=32)\n",
    "parser.add_argument('--train_random_seed',type=int, default=42)\n",
    "parser.add_argument('--test_random_seed',type=int, default=43)\n",
    "\n",
    "opt = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c17e33b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fff2f53",
   "metadata": {},
   "source": [
    "data preprocssing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69420bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTProcessor():\n",
    "    \n",
    "    '''signal to FFT image'''\n",
    "    \n",
    "    def __init__(self, sampling_frequency, img_x_dir, img_y_dir, random_seed):\n",
    "        self.fs = sampling_frequency\n",
    "        self.img_x_dir = img_x_dir\n",
    "        self.img_y_dir = img_y_dir\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "    def generate_random_signal(self, length):\n",
    "        t = np.linspace(0, length, int(self.fs), endpoint=False)\n",
    "        x = np.zeros_like(t)\n",
    "        \n",
    "        num_components = np.random.randint(20, 50)  # 2~4\n",
    "        for _ in range(num_components):\n",
    "            A = np.random.uniform(0.5, 3.0)            # 진폭\n",
    "            f = np.random.uniform(1, 20)               # 주파수 (Hz)\n",
    "            phi = np.random.uniform(0, 2*np.pi)        # 위상\n",
    "            x += A * np.sin(2 * np.pi * f * t + phi)\n",
    "        return x\n",
    "\n",
    "    def train_process_batch(self, n_samples, patch_size, stride):\n",
    "        for sample_idx in range(n_samples):\n",
    "            np.random.seed(self.random_seed)\n",
    "            x = self.generate_random_signal(opt.lr_length)  # 이렇게 opt. 쓰는 것?? 별론가\n",
    "            y = self.generate_random_signal(opt.hr_length)\n",
    "\n",
    "            X = np.fft.fft(x)\n",
    "            Y = np.fft.fft(y)\n",
    "            #freqs = np.fft.fftfreq(len(x), d=1/self.fs)\n",
    "            mag_X = np.abs(X)[:len(X)//2]\n",
    "            mag_Y = np.abs(Y)[:len(Y)//2]\n",
    "            tiled_X = np.tile(mag_X, (patch_size, 1))\n",
    "            tiled_Y = np.tile(mag_Y, (patch_size, 1)) \n",
    "            \n",
    "            plt.imsave(f\"{self.img_x_dir}/fft_{sample_idx:03d}_wholeX.png\", tiled_X, cmap='gray')\n",
    "            plt.imsave(f\"{self.img_x_dir}/fft_{sample_idx:03d}_wholeY.png\", tiled_Y, cmap='gray')\n",
    "\n",
    "            patch_idx = 0\n",
    "            for i in range(0, tiled_X.shape[0] - patch_size + 1, stride):\n",
    "                for j in range(0, tiled_X.shape[1] - patch_size + 1, stride):\n",
    "                    patch = tiled_X[i:i+patch_size, j:j+patch_size]\n",
    "                    plt.imsave(f\"{self.img_x_dir}/fft_{sample_idx:03d}_{patch_idx:02d}.png\", patch, cmap='gray')\n",
    "                    patch_idx += 1\n",
    "                    \n",
    "            patch_idx = 0\n",
    "            for i in range(0, tiled_Y.shape[0] - patch_size + 1, stride):\n",
    "                for j in range(0, tiled_Y.shape[1] - patch_size + 1, stride):\n",
    "                    patch = tiled_Y[i:i+patch_size, j:j+patch_size]\n",
    "                    plt.imsave(f\"{self.img_y_dir}/fft_{sample_idx:03d}_{patch_idx:02d}.png\", patch, cmap='gray')\n",
    "                    patch_idx += 1\n",
    "                    \n",
    "        \n",
    "\n",
    "    def test_process_batch(self, n_samples):\n",
    "        for idx in range(n_samples):\n",
    "            np.random.seed(self.random_seed)\n",
    "            x = self.generate_random_signal(opt.lr_length)\n",
    "            y = self.generate_random_signal(opt.hr_length)\n",
    "\n",
    "            X = np.fft.fft(x)\n",
    "            Y = np.fft.fft(y)\n",
    "            #freqs = np.fft.fftfreq(len(x), d=1/self.fs)\n",
    "            mag_X = np.abs(X)[:len(X)//2]\n",
    "            mag_Y = np.abs(Y)[:len(Y)//2]\n",
    "            #tiled_X = np.tile(mag_X, (opt.patch_size, 1))\n",
    "            tiled_X = np.tile(mag_X, (opt.hr_length, 1))\n",
    "            tiled_Y = np.tile(mag_Y, (opt.hr_length, 1)) \n",
    "\n",
    "            plt.imsave(f\"{self.img_x_dir}/fft_{idx:03d}.png\", tiled_X, cmap='gray')\n",
    "            plt.imsave(f\"{self.img_y_dir}/fft_{idx:03d}.png\", tiled_Y, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e33326ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_patch(x, img_dir, idx):\n",
    "    \n",
    "    plt.plot(x[:, 0], x[:, 1])\n",
    "    plt.axis('off')     # 축 숨기기 (x, y 모두)\n",
    "    plt.gca().spines['top'].set_visible(False)    # 테두리(스파인) 숨기기\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.gca().spines['bottom'].set_visible(False)\n",
    "    plt.gca().spines['left'].set_visible(False)\n",
    "    plt.savefig(f\"{img_dir}/fft_{idx:03d}.png\", bbox_inches = 'tight', transparent = True)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cf3ea7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST\n",
    "\n",
    "class FFTProcessor():\n",
    "    \n",
    "    '''signal to FFT image'''\n",
    "    \n",
    "    def __init__(self, sampling_frequency, img_x_dir, img_y_dir, random_seed):\n",
    "        self.fs = sampling_frequency\n",
    "        self.img_x_dir = img_x_dir\n",
    "        self.img_y_dir = img_y_dir\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "    def generate_random_signal(self, length):\n",
    "        t = np.linspace(0, length, int(self.fs), endpoint=False)\n",
    "        x = np.zeros_like(t)\n",
    "        \n",
    "        num_components = np.random.randint(20, 50)  # 2~4\n",
    "        for _ in range(num_components):\n",
    "            A = np.random.uniform(0.5, 3.0)            # 진폭\n",
    "            f = np.random.uniform(1, 20)               # 주파수 (Hz)\n",
    "            phi = np.random.uniform(0, 2*np.pi)        # 위상\n",
    "            x += A * np.sin(2 * np.pi * f * t + phi)\n",
    "        return x\n",
    "\n",
    "    def train_process_batch(self, n_samples, patch_size, stride):\n",
    "        for sample_idx in range(n_samples):\n",
    "            np.random.seed(self.random_seed)\n",
    "            x = self.generate_random_signal(opt.lr_length)  # 이렇게 opt. 쓰는 것?? 별론가\n",
    "            y = self.generate_random_signal(opt.hr_length)\n",
    "\n",
    "            X = np.fft.fft(x)\n",
    "            Y = np.fft.fft(y)\n",
    "            freqs = np.fft.fftfreq(len(X), d=1/self.fs)\n",
    "            mag_X = np.abs(X)* 2 / len(X)\n",
    "            mag_Y = np.abs(Y)* 2 / len(Y)\n",
    "            freqs = np.fft.fftfreq(len(X), d=1/self.fs)\n",
    "        \n",
    "            half = int(len(X)/2)\n",
    "            freq_mag_X = np.column_stack((freqs[:half], mag_X[:half]))\n",
    "            freq_mag_Y = np.column_stack((freqs[:half], mag_Y[:half]))\n",
    "            \n",
    "            save_patch(freq_mag_X, self.img_x_dir, sample_idx)\n",
    "            save_patch(freq_mag_Y, self.img_y_dir, sample_idx)\n",
    "\n",
    "                \n",
    "    def test_process_batch(self, n_samples):\n",
    "        for idx in range(n_samples):\n",
    "            np.random.seed(self.random_seed)\n",
    "            x = self.generate_random_signal(opt.lr_length)\n",
    "            y = self.generate_random_signal(opt.hr_length)\n",
    "\n",
    "            X = np.fft.fft(x)\n",
    "            Y = np.fft.fft(y)\n",
    "            #freqs = np.fft.fftfreq(len(x), d=1/self.fs)\n",
    "            mag_X = np.abs(X)[:len(X)//2]\n",
    "            mag_Y = np.abs(Y)[:len(Y)//2]\n",
    "\n",
    "            #plt.imsave(f\"{self.img_x_dir}/fft_{idx:03d}.png\", mag_X, cmap='gray')\n",
    "            #plt.imsave(f\"{self.img_y_dir}/fft_{idx:03d}.png\", mag_Y, cmap='gray')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "80be3a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FFTProcessor(opt.sampling_frequency, opt.train_x_img_dir, opt.train_y_img_dir, opt.train_random_seed).train_process_batch(opt.n_samples, opt.patch_size, opt.stride)\n",
    "FFTProcessor(opt.sampling_frequency, opt.test_x_img_dir, opt.test_y_img_dir, opt.test_random_seed).test_process_batch(opt.n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4184e18d",
   "metadata": {},
   "source": [
    "data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "32f00ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_dataset():\n",
    "    def __init__(self, train_x, train_y, test_x, test_y):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.test_x = test_x\n",
    "        self.test_y = test_y\n",
    "        \n",
    "    def main(self):\n",
    "        self.to_tensor()\n",
    "        return self.train_dl, self.test_dl\n",
    "                \n",
    "    def to_tensor(self):\n",
    "        # To tensor\n",
    "        train_x = torch.tensor(self.train_x)\n",
    "        train_y = torch.tensor(self.train_y)\n",
    "        test_x = torch.tensor(self.test_x)\n",
    "        test_y = torch.tensor(self.test_y)\n",
    "\n",
    "        train = MyDataset(train_x, train_y)\n",
    "        test = MyDataset(test_x, test_y)\n",
    "\n",
    "        self.train_dl = DataLoader(train, opt.batch_size, shuffle=True)\n",
    "        self.test_dl = DataLoader(test, opt.batch_size, shuffle=False)\n",
    "    \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.x = self.x.unsqueeze(1)\n",
    "        self.y = torch.argmax(y, 1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        x = self.x[ix]\n",
    "        y = self.y[ix]\n",
    "        return x.to(device).float(), y.to(device).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "52336b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\limuri\\anaconda3\\envs\\superi\\Lib\\site-packages\\PIL\\Image.py:3402: DecompressionBombWarning: Image size (100000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_x = []\n",
    "for filename in os.listdir('./data/train/x'):\n",
    "    img_path = os.path.join('./data/train/x', filename)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_array = np.array(img)\n",
    "    train_x.append(img_array)\n",
    "    \n",
    "train_y = []\n",
    "for filename in os.listdir('./data/train/y'):\n",
    "    img_path = os.path.join('./data/train/y', filename)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_array = np.array(img)\n",
    "    train_y.append(img_array)\n",
    "    \n",
    "test_x = []\n",
    "for filename in os.listdir('./data/test/x'):\n",
    "    img_path = os.path.join('./data/test/x', filename)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_array = np.array(img)\n",
    "    test_x.append(img_array)\n",
    "    \n",
    "test_y = []\n",
    "for filename in os.listdir('./data/test/y'):\n",
    "    img_path = os.path.join('./data/test/y', filename)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_array = np.array(img)\n",
    "    test_y.append(img_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6de58ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limuri\\AppData\\Local\\Temp\\ipykernel_6408\\50614297.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  train_x = torch.tensor(self.train_x)\n"
     ]
    }
   ],
   "source": [
    "train_dl, test_dl = make_dataset(train_x, train_y, test_x, test_y).main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861fb37e",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "59d9ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN(nn.Module):\n",
    "    def __init__(self, num_channels=1): #흑백\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=9, padding = 9//2) #(H, W) \n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=5, padding = 5//2)\n",
    "        self.conv3 = nn.Conv2d(32, num_channels, kernel_size=5, padding = 5//2)\n",
    "        self.relu = nn.ReLU\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.relu(self.conv1(x))\n",
    "        x=self.relu(self.conv2(x))\n",
    "        x=self.conv3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6765fa5",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5ef1a334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"            \\n        if epoch_valid < best_loss:\\n            best_model = model\\n            early_stopping_count = 0\\n            torch.save(model, result_dir + '/model.pt')\\n            dict_result['stop_point'] = epoch+1\\n            print( ' >>> Best model save ! '.format(epoch+1, epoch_valid))\\n            \\n            best_loss = epoch_valid\\n\\n        else:\\n            early_stopping_count = early_stopping_count + 1\\n\\n        if early_stopping_count > opt.early_stop:\\n            break\\n           \\n    with open(result_dir + '/result.pickle', 'wb') as f:\\n        pickle.dump(dict_result, f)\\n        \""
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train(opt, train_dl):\n",
    "    \n",
    "    dict_result = {'train_loss': [],\n",
    "                   'stop_point': 0\n",
    "                    }\n",
    "    \n",
    "    best_loss = 10000\n",
    "    early_stopping_count = 0\n",
    "    \n",
    "    #model = Model().to(device)\n",
    "    model = SRCNN().to(device)\n",
    "    loss_cross = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()), lr=opt.lr)\n",
    "    #scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=lambda epoch: opt.lamda ** epoch)\n",
    "    \n",
    "    pbar = tqdm(range(opt.epochs), unit = 'epoch')\n",
    "    \n",
    "    for epoch in pbar:\n",
    "        iter_loss = []\n",
    "        #iter_valid_loss = []\n",
    "        \n",
    "        # Training Model\n",
    "        model.train()\n",
    "        for ix, batch in enumerate(iter(train_dl)):\n",
    "            x, y = batch\n",
    "            pred = model(x)\n",
    "            loss_y = loss_cross(pred, y)\n",
    "\n",
    "            batch_loss = loss_y\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            iter_loss.append(batch_loss.item())\n",
    "            \n",
    "            pbar.set_postfix({'train_loss' : loss_y.item()})\n",
    "\n",
    "    #    scheduler.step()\n",
    "        '''\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for ix, batch in enumerate(iter(valid_dl)):\n",
    "                v_x, v_y = batch\n",
    "                v_pred = model(v_x)\n",
    "                v_loss_y = loss_cross(v_pred, v_y)\n",
    "                \n",
    "                iter_valid_loss.append(v_loss_y.item())\n",
    "'''\n",
    "        epoch_train = np.mean(iter_loss)\n",
    "        #epoch_valid = np.mean(iter_valid_loss)\n",
    "\n",
    "        #print( ' Epoch: {} / train_loss: {:.4f} / valid_loss: {:.4f}'.format(epoch+1, epoch_train, epoch_valid))\n",
    "        print( ' Epoch: {} / train_loss: {:.4f} / valid_loss: {:.4f}'.format(epoch+1, epoch_train))\n",
    "            \n",
    "        ##################### result_save ############################           \n",
    "        dict_result['train_loss'].append(epoch_train)\n",
    "        #dict_result['valid_loss'].append(epoch_valid)\n",
    "        ###############################################################\n",
    "'''            \n",
    "        if epoch_valid < best_loss:\n",
    "            best_model = model\n",
    "            early_stopping_count = 0\n",
    "            torch.save(model, result_dir + '/model.pt')\n",
    "            dict_result['stop_point'] = epoch+1\n",
    "            print( ' >>> Best model save ! '.format(epoch+1, epoch_valid))\n",
    "            \n",
    "            best_loss = epoch_valid\n",
    "\n",
    "        else:\n",
    "            early_stopping_count = early_stopping_count + 1\n",
    "\n",
    "        if early_stopping_count > opt.early_stop:\n",
    "            break\n",
    "           \n",
    "    with open(result_dir + '/result.pickle', 'wb') as f:\n",
    "        pickle.dump(dict_result, f)\n",
    "        '''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daadf27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x17d237870b0>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "84fb1c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9207de126dd64b2db7682fc2dbcbf0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [206, 1, 128, 128, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[184], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(opt, train_dl)\n",
      "Cell \u001b[1;32mIn[183], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(opt, train_dl)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ix, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_dl)):\n\u001b[0;32m     25\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m---> 26\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m     27\u001b[0m     loss_y \u001b[38;5;241m=\u001b[39m loss_cross(pred, y)\n\u001b[0;32m     29\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m loss_y\n",
      "File \u001b[1;32mc:\\Users\\limuri\\anaconda3\\envs\\superi\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\limuri\\anaconda3\\envs\\superi\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[174], line 11\u001b[0m, in \u001b[0;36mSRCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x))\n\u001b[0;32m     12\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[0;32m     13\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)\n",
      "File \u001b[1;32mc:\\Users\\limuri\\anaconda3\\envs\\superi\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\limuri\\anaconda3\\envs\\superi\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\limuri\\anaconda3\\envs\\superi\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32mc:\\Users\\limuri\\anaconda3\\envs\\superi\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    551\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [206, 1, 128, 128, 3]"
     ]
    }
   ],
   "source": [
    "train(opt, train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a097981",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a87ecce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "superi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
